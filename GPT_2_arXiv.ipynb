{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT-2_arXiv",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01aabbf275b94c65a22cf86f735cf217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9ecf82aa7594f72a5a3c8aeaa903029",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62ff7b0901f24b0ca110e4335a023be0",
              "IPY_MODEL_3d026e68adca470387a5d79bce192564"
            ]
          }
        },
        "a9ecf82aa7594f72a5a3c8aeaa903029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "62ff7b0901f24b0ca110e4335a023be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a568e8ea0e254a99b175be7ed5b1c428",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1168181,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1168181,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cbbe34dc83cc410faeba83d847faa605"
          }
        },
        "3d026e68adca470387a5d79bce192564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3818869c725549ef8d6bc6b65a1659f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1168181/1168181 [00:41&lt;00:00, 28211.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79b9404a478f4b5ba10a7b6028b873fb"
          }
        },
        "a568e8ea0e254a99b175be7ed5b1c428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cbbe34dc83cc410faeba83d847faa605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3818869c725549ef8d6bc6b65a1659f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79b9404a478f4b5ba10a7b6028b873fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "308d81ea6c1a47dc87c01a90dfa0c336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9139c1adc8cd43a39f151a1f29ebf76e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ac0fd36b8d5432db6404f4d44bd7b9e",
              "IPY_MODEL_719059f6158c4bd8bfafcdd27c5406b0"
            ]
          }
        },
        "9139c1adc8cd43a39f151a1f29ebf76e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "9ac0fd36b8d5432db6404f4d44bd7b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eea1465249b34703a02349428cfe4941",
            "_dom_classes": [],
            "description": "Loss: 3.322 â€” Avg: 3.329 â€” GPU Mem: 9211 MB: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 20000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d65a623126cd4bf9b76c9e121b356bf6"
          }
        },
        "719059f6158c4bd8bfafcdd27c5406b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29f1b26741124f5da28e82f94d534378",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20000/20000 [1:25:21&lt;00:00,  3.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83a8bdb11549415ebb8ea516d35327b2"
          }
        },
        "eea1465249b34703a02349428cfe4941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d65a623126cd4bf9b76c9e121b356bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29f1b26741124f5da28e82f94d534378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83a8bdb11549415ebb8ea516d35327b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJX_nTQogJp6"
      },
      "source": [
        "# GPT-2 arXiv Title Generator\n",
        "\n",
        "## tl;dr\n",
        "1. `Connect` or `Reconnect`\n",
        "2. Upload your Kaggle API key (instructions included later)\n",
        "3. `Runtime` -> `Restart and run all`\n",
        "4. Wait for about 90 minutes\n",
        "5. Laugh at weird computer-generated headlines\n",
        "\n",
        "\n",
        "by Brian Lechthaler, \n",
        "*based on [aitextgen](https://github.com/minimaxir/aitextgen)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dMTiyLIgnAc",
        "outputId": "c629a10f-c5ab-4b50-ad7a-309843952366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from datetime import datetime\n",
        "def mktimestamp():\n",
        "  timestamp = datetime.now()\n",
        "  msg = \"Last Updated: \" + str(timestamp)\n",
        "  return msg\n",
        "print(mktimestamp())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last Updated: 2020-11-13 22:07:03.662976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpAi6OHkIoB",
        "outputId": "51eedd8f-c60f-40fe-9700-234b8c4a158d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 13 22:07:04 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8tXdKjUhard"
      },
      "source": [
        "# Dependencies\n",
        "Download and install all necessary dependencies with `pip`, then `import` what we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIJGHezJewYk",
        "outputId": "b103ea7a-bda7-445a-b329-256348c88500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Freeze versions of dependencies for now\n",
        "!pip install -q transformers==2.9.1\n",
        "!pip install -q pytorch-lightning==0.7.6\n",
        "\n",
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s â€” %(levelname)s â€” %(name)s â€” %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\n",
        "from aitextgen.TokenDataset import TokenDataset, merge_datasets\n",
        "from aitextgen.utils import build_gpt2_config\n",
        "from aitextgen.tokenizers import train_tokenizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 22:07:12 â€” INFO â€” transformers.file_utils â€” PyTorch version 1.7.0+cu101 available.\n",
            "11/13/2020 22:07:13 â€” INFO â€” transformers.file_utils â€” TensorFlow version 2.3.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w9n5ZkShpY9"
      },
      "source": [
        "# Mount Google Drive\n",
        "Because any data in the VM this notebook is running on will be nuked once the Jupyter kernel stops running, it's helpful to mount your Google Drive to the Colab VM to persist some files that we'll use in this notebook.\n",
        "\n",
        "*Note:* your data will not be shared with anyone who does not have direct access to the VM running this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfDol0OZfAPq",
        "outputId": "485806b3-5520-4340-801c-fb9fba33f5d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r7Q-truvtgv"
      },
      "source": [
        "# Download Dataset from Kaggle\n",
        "Downloads the 'arxiv' dataset contributed by Kaggle user `Cornell-University`\n",
        "\n",
        "1.   Sign into Kaggle in a separate tab\n",
        "2.   Click [this link](https://kaggle.com/me/account) to go to your Kaggle account settings\n",
        "3. Under the `API` section, click/tap `Create new API token`. If this is not the first time you have followed this step, consider clicking `Expire API Token` prior to generating a new token.\n",
        "4. In the Colab file browser, upload the `kaggle.json` API token you just downloaded in step 3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2x4M0GMrHTQ"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEv2I3USrkiY",
        "outputId": "d1494cb7-55b0-48c5-9c4e-98fe162751a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!rm -rf arxiv.zip\n",
        "!kaggle datasets download -d Cornell-University/arxiv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading arxiv.zip to /content\n",
            " 97% 874M/902M [00:10<00:00, 115MB/s] \n",
            "100% 902M/902M [00:10<00:00, 93.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47PdWd5qPdwU",
        "outputId": "0a592580-025a-4c7a-8ebe-8de3551abb1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf arxiv-metadata-oai-snapshot.json\n",
        "!unzip arxiv.zip\n",
        "dataset_original = 'arxiv-metadata-oai-snapshot.json'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  arxiv.zip\n",
            "  inflating: arxiv-metadata-oai-snapshot.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpTYzc74yW20"
      },
      "source": [
        "# Transform Dataset\n",
        "We need to define a couple functions to drop unnecessary columns from our data before using it to finetune GPT-2. In addition to dropping all columns except exactly what we need, we also randomly sample 45% of the data so that our dataset will reliably fit in GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSiUS4G4Pn--"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QVCB4OHaAiw"
      },
      "source": [
        "def writeln(line, path):\n",
        "  line = line + \"\\n\"\n",
        "  with open(path, 'a') as saveto:\n",
        "    saveto.write(line)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UteGdkjXafm"
      },
      "source": [
        "def finalsave(df, colname, filename, sample_frac):\n",
        "  print(\"Transforming dataset...\")\n",
        "  df = df.sample(frac=sample_frac)\n",
        "  for index, row in df.iterrows():\n",
        "    line = row[colname]\n",
        "    writeln(line, filename)\n",
        "  print(\"Done!\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXXTbC2iLOM6"
      },
      "source": [
        "*Important Note:* If any cell below this message crashes your Colab runtime, you probably ran out of memory. Sorry, but if the problem persists you may need to shell out $10 to Google for Colab Pro and change the runtime to GPU High RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuxO5d2APtSe",
        "outputId": "ecfa86d0-e4d2-4e63-b2d3-ea25c2745fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jsoningest = pd.read_json(dataset_original, lines=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 22:09:01 â€” INFO â€” numexpr.utils â€” NumExpr defaulting to 4 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpDVwLJ_d7eL",
        "outputId": "ef184d76-a1db-47a9-9cb7-ad4d914e496c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf /content/dataset.csv\n",
        "!touch /content/dataset.csv\n",
        "file_name = '/content/dataset.csv'\n",
        "finalsave(jsoningest, \n",
        "          'title', \n",
        "          file_name,\n",
        "          0.45)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming dataset...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK9GbJGpVMRS"
      },
      "source": [
        "def cleardf(df):\n",
        "  print('Emptying Pandas DataFrame...')\n",
        "  df = pd.DataFrame()\n",
        "  print('Done!')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIr2vhfHVf2m",
        "outputId": "6b75f889-cee4-4cf1-b721-38c8057b9a9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cleardf(jsoningest)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emptying Pandas DataFrame...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCP7Yr_VibKI"
      },
      "source": [
        "# Train the Tokenizer\n",
        "This runs on the CPU and will take a while.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVrLbVCohZ7",
        "outputId": "47988906-da7d-4ea0-be4c-5191a7461749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_name = '/content/dataset.csv'\n",
        "!rm -rf aitextgen-merges.txt\n",
        "!rm -rf aitextgen-vocab.json\n",
        "!rm -rf trained_model\n",
        "train_tokenizer(file_name)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 22:11:31 â€” INFO â€” aitextgen.tokenizers â€” Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tixuti80omSE"
      },
      "source": [
        "# Configure GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSZ4m8yTopQr",
        "outputId": "bb27fd12-f71c-43e0-f7e7-577a34307af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "config = build_gpt2_config(vocab_size=60000, \n",
        "                           max_length=20,\n",
        "                           dropout=0.0, \n",
        "                           n_embd=256, \n",
        "                           n_layer=16, \n",
        "                           n_head=16)\n",
        "config"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.0,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"embd_pdrop\": 0.0,\n",
              "  \"eos_token_id\": 0,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_ctx\": 20,\n",
              "  \"n_embd\": 256,\n",
              "  \"n_head\": 16,\n",
              "  \"n_layer\": 16,\n",
              "  \"n_positions\": 20,\n",
              "  \"resid_pdrop\": 0.0,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.0,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"vocab_size\": 60000\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbsF9Owos1b",
        "outputId": "ed027b2a-5c25-467c-dd80-32fbf503e7c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ai = aitextgen(config=config,\n",
        "               vocab_file=\"aitextgen-vocab.json\",\n",
        "               merges_file=\"aitextgen-merges.txt\",\n",
        "               to_gpu=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 22:11:31 â€” INFO â€” aitextgen â€” Constructing GPT-2 model from provided config.\n",
            "11/13/2020 22:11:32 â€” INFO â€” aitextgen â€” Using a custom tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXVGip1HoxNM"
      },
      "source": [
        "# Finetune GPT-2 to dataset\n",
        "Training should take about an hour and a half on an NVidia Tesla P100 GPU. Text generated from the model should get progressively better over iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aamon3x-owP3",
        "outputId": "57cf392f-cdb2-4c7e-bc4d-c300ebc53834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "01aabbf275b94c65a22cf86f735cf217",
            "a9ecf82aa7594f72a5a3c8aeaa903029",
            "62ff7b0901f24b0ca110e4335a023be0",
            "3d026e68adca470387a5d79bce192564",
            "a568e8ea0e254a99b175be7ed5b1c428",
            "cbbe34dc83cc410faeba83d847faa605",
            "3818869c725549ef8d6bc6b65a1659f3",
            "79b9404a478f4b5ba10a7b6028b873fb",
            "308d81ea6c1a47dc87c01a90dfa0c336",
            "9139c1adc8cd43a39f151a1f29ebf76e",
            "9ac0fd36b8d5432db6404f4d44bd7b9e",
            "719059f6158c4bd8bfafcdd27c5406b0",
            "eea1465249b34703a02349428cfe4941",
            "d65a623126cd4bf9b76c9e121b356bf6",
            "29f1b26741124f5da28e82f94d534378",
            "83a8bdb11549415ebb8ea516d35327b2"
          ]
        }
      },
      "source": [
        "!rm -rf trained_model\n",
        "!nvidia-smi\n",
        "ai.train(file_name,\n",
        "         line_by_line=True,\n",
        "         num_steps=20000,\n",
        "         generate_every=500,\n",
        "         save_every=100,\n",
        "         save_gdrive=True,\n",
        "         learning_rate=1e-4,\n",
        "         batch_size=256)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 13 22:11:38 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    37W / 300W |   1463MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01aabbf275b94c65a22cf86f735cf217",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1168181.0), HTML(value='')), layout=Layouâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 22:11:39 â€” INFO â€” aitextgen.TokenDataset â€” Encoding 1,168,181 rows from /content/dataset.csv.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "11/13/2020 22:12:22 â€” INFO â€” lightning â€” GPU available: True, used: True\n",
            "No environment variable for node rank defined. Set as 0.\n",
            "11/13/2020 22:12:22 â€” WARNING â€” lightning â€” No environment variable for node rank defined. Set as 0.\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "11/13/2020 22:12:22 â€” INFO â€” lightning â€” CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308d81ea6c1a47dc87c01a90dfa0c336",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=20000.0), HTML(value='')), layout=Layout(â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  SEPI7\n",
            "==========\n",
            "\u001b[1m600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "The $G(T)_2$ and 2)\n",
            "==========\n",
            "\u001b[1m1,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "A Baler-Zadchet Model for Computers of Snenle\n",
            "==========\n",
            "\u001b[1m1,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m1,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Grided Terenting Affer-Based Surveys\n",
            "==========\n",
            "\u001b[1m2,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "The X-ray X-Ray Bursts X-ray 157+782:\n",
            "==========\n",
            "\u001b[1m2,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m2,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  and tangent works\n",
            "==========\n",
            "\u001b[1m3,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  with high-mass galaxies\n",
            "==========\n",
            "\u001b[1m3,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m3,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  the Eigenvalue and Six-Hands\n",
            "==========\n",
            "\u001b[1m4,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "A model for the template morphology of X-ray observations\n",
            "==========\n",
            "\u001b[1m4,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m4,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  the Dilaton Golds\n",
            "==========\n",
            "\u001b[1m5,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  the Laser-Kinematic\n",
            "==========\n",
            "\u001b[1m5,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m5,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  in the two-dimensional nonlinear Hubbard model\n",
            "==========\n",
            "\u001b[1m6,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  Can be Parsec-in-Lifshitz symmetry\n",
            "==========\n",
            "\u001b[1m6,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m6,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Braneworlds and L-functions\n",
            "==========\n",
            "\u001b[1m7,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Breathing in the sine-Gordon system with a strong coupling\n",
            "==========\n",
            "\u001b[1m7,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m7,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  in the Athena Luminosity Function\n",
            "==========\n",
            "\u001b[1m8,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  in a 2+1 dimensions\n",
            "==========\n",
            "\u001b[1m8,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m8,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Meson-polarized quark matter and the scalar field\n",
            "==========\n",
            "\u001b[1m9,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Laser-induced magnetic resonance of a dipolar Fermi gas in a two-dimensional\n",
            "==========\n",
            "\u001b[1m9,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m9,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  (111)\n",
            "==========\n",
            "\u001b[1m10,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "On the Limiting Eigenvalue of the Sierpinski-Kol\n",
            "==========\n",
            "\u001b[1m10,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m10,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  and MnSi films\n",
            "==========\n",
            "\u001b[1m11,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  equations\n",
            "==========\n",
            "\u001b[1m11,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m11,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  for non-hermitian systems\n",
            "==========\n",
            "\u001b[1m12,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  and stirred observations\n",
            "==========\n",
            "\u001b[1m12,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m12,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Symmetries in N=4 supersymmetric Yang-Mills theory\n",
            "==========\n",
            "\u001b[1m13,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Turbulence in the Universe\n",
            "==========\n",
            "\u001b[1m13,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m13,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  non-vanishing of Borel sums\n",
            "==========\n",
            "\u001b[1m14,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  and applications\n",
            "==========\n",
            "\u001b[1m14,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m14,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Limits on the WIMP-nucleus scattering from the proton\n",
            "==========\n",
            "\u001b[1m15,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  Retrospective\n",
            "==========\n",
            "\u001b[1m15,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m15,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  and $H^{1}_{p}$\n",
            "==========\n",
            "\u001b[1m16,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  in the final state\n",
            "==========\n",
            "\u001b[1m16,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m16,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  pumping\n",
            "==========\n",
            "\u001b[1m17,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "The paper \"Cayley-Solitar\n",
            "==========\n",
            "\u001b[1m17,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m17,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  Inclusive Dilepton Production at the LHC\n",
            "==========\n",
            "\u001b[1m18,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "Smooth extensions of the Toda lattice and the Lick Bergman space for\n",
            "==========\n",
            "\u001b[1m18,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m18,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  with missing transverse momentum\n",
            "==========\n",
            "\u001b[1m19,100 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,200 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,300 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,400 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,500 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "The Nature of the Stellar Mass Functions in the LS I Clusters of Galaxies\n",
            "==========\n",
            "\u001b[1m19,600 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,700 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,800 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m19,900 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m20,000 steps reached: saving model to /trained_model\u001b[0m\n",
            "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "  tetrahedron\n",
            "==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "11/13/2020 23:37:34 â€” INFO â€” aitextgen â€” Saving trained model pytorch_model.bin to /trained_model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0kW0lpZo85M"
      },
      "source": [
        "# Generate a few Samples\n",
        "Now, for the fun part! Before I continue, I want to be really clear: all headlines you see in this notebook are 100% fake and generated by GPT-2.  Please use this code responsibly (that means never use this to intentionally decieve people, generate clickbait, or do anything else blackhatty)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9YEwLVHpAYO",
        "outputId": "73d7c9d5-d473-4a14-98f6-ebc69385c2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ai.generate(n=25,\n",
        "            batch_size=512,\n",
        "            temperature=1.0,\n",
        "            top_p=0.999)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  from muonium in Pb-Pb collisions at LDA\n",
            "==========\n",
            "Sparticle formation from fragmentation of a single and dutiny-parity\n",
            "==========\n",
            "Unimodality of multiplicative nash\n",
            "==========\n",
            "Antiproton cross sections at high energy in the T1 picture and\n",
            "==========\n",
            "  in $\\dot{\\mathrm{P}^2$\n",
            "==========\n",
            "New explicit and exactly blow-up to the 2D XY and Ahar\n",
            "==========\n",
            "Accurate estimation of the power spectrum of the Hubble Frontier: the\n",
            "==========\n",
            "  \\leq e^\\cy \\ln \\infty$-Borel\n",
            "==========\n",
            "Autonomous quantum dots in strong laser fields via bending of laser\n",
            "==========\n",
            "  model\n",
            "==========\n",
            "  Networks\n",
            "==========\n",
            "  campus labeled with the Parkes cloud\n",
            "==========\n",
            "Communication Latent Variable Network Models for Medical Image Sequences\n",
            "==========\n",
            "  with non-constant Ricci curvature\n",
            "==========\n",
            "  (Leptofire)\n",
            "==========\n",
            "Antiferromagnetism in the Hubbard model\n",
            "==========\n",
            "Change and Stopping Properties of the CDF2 Inspired by Wi-\n",
            "==========\n",
            "Public Key Supports in Smart People using Health M\n",
            "==========\n",
            "  supersingular domains\n",
            "==========\n",
            "Loss of the first kinematics of Galactic bulge\n",
            "==========\n",
            "  CeO$_{5-\\delta}$\n",
            "==========\n",
            "  Gravitated Geometry\n",
            "==========\n",
            "Precise Prediction of Wheeled Carbon Abundance with XMM-Newton X\n",
            "==========\n",
            "The Kinetics of Folded Black Hole Thin Films\n",
            "==========\n",
            "Building Bernstein and Braid Cells of a Class and Punctured\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr98L6xC5jQc",
        "outputId": "255e86fa-399a-47bd-94e9-daf8eda53047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 13 23:37:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    38W / 300W |   9241MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMs-joQL0PRs"
      },
      "source": [
        "# Credits\n",
        "\n",
        "This project was made possible by the cumulative efforts of the following parties:\n",
        "\n",
        "Brian Lechthaler *author of this notebook*\n",
        "* https://github.com/brianlechthaler\n",
        "* https://twitter.com/brianlechthaler\n",
        "\n",
        "Max Woolf *author of [aitextgen](https://github.com/minimaxir/aitextgen), the training code this notebook is based on.*\n",
        "* https://minimaxir.com/\n",
        "* https://github.com/minimaxir\n",
        "\n",
        "Cornell University *author of [arxiv](https://www.kaggle.com/Cornell-University/arxiv) dataset*\n",
        "* https://www.cornell.edu/\n",
        "* https://www.kaggle.com/Cornell-University\n",
        "\n",
        "OpenAI *creators of [GPT-2](https://en.wikipedia.org/wiki/OpenAI#GPT-2) model*\n",
        "* https://openai.com \n",
        "* https://openai.com/blog/tags/gpt-2/\n"
      ]
    }
  ]
}